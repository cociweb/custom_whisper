---
configuration:
  beam_size:
    name: Beam size
    description: >-
      Number of candidates to consider simultaneously during transcription.
      Increasing the beam size will increase accuracy at the cost of
      performance.
  compute_type:
    name: Computation type
    description: >-
       The translator exposes the option `compute_type` that accepts values. The default value is `default`: Best fits for the model and the device.
       Reference: https://opennmt.net/CTranslate2/quantization.html#quantize-on-model-loading
  language:
    name: Language
    description: >-
      Language that you will speak to the add-on. If you select "auto",
      the model will run much slower but will auto-detect the spoken language.
  model:
    name: Model
    description: |
      Whisper model that will be used for transcription.

      The default model is `tiny-int8`, a compressed version of the smallest
      Whisper model which is able to run on a Raspberry Pi 4. Compressed models
      (`int8`) are slightly less accurate than their counterparts, but smaller
      and faster.
  custom_model_name:
    name: Custom model name
    description: |
      Name of a customized Whisper model that will be used for transcription.

      To take into account the `model` should be `custom` and url should be definied!
  custom_model_url:
    name: URL of Custom model repository.
    description: |
      URL of a customized Whisper model that will be used for transcription.

      The repository should contain at least the following files: `model.bin`,
      `vocabulary.txt`, `config.json`. Additionaly a `hash.json` is required with md5sum hash code of each files.

      Don't forget to append the `/resolve/{branch}/` at the end!
      Eg: `https://huggingface.co/Hungarians/faster-whisper-tiny-cv16-int8.hu/resolve/main/`
network:
  10300/tcp: Whisper Wyoming Protocol
